{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "from datasets import load_dataset\n",
    "import pprint\n",
    "import logging\n",
    "\n",
    "# from utilities import *\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"lamini_docs.jsonl\"\n",
    "dataset_path = f\"{dataset_name}\"\n",
    "use_hf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    if \"question\" in examples and \"answer\" in examples:\n",
    "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "    elif \"input\" in examples and \"output\" in examples:\n",
    "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "    else:\n",
    "      text = examples[\"text\"][0]\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_loaded = load_dataset(\"json\", data_files=dataset_name, split=\"train\")\n",
    "\n",
    "tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"What are the different types of documents available in the repository (e.g., installation guide, API documentation, developer's guide)?\", 'answer': 'Lamini has documentation on Getting Started, Authentication, Question Answer Model, Python Library, Batching, Error Handling, Advanced topics, and class documentation on LLM Engine available at https://lamini-ai.github.io/.', 'input_ids': [1276, 403, 253, 1027, 3510, 273, 7177, 2130, 275, 253, 18491, 313, 70, 15, 72, 904, 12692, 7102, 13, 8990, 10097, 13, 13722, 434, 7102, 6177, 45, 4988, 74, 556, 10097, 327, 27669, 11075, 264, 13, 5271, 23058, 13, 19782, 37741, 10031, 13, 13814, 11397, 13, 378, 16464, 13, 11759, 10535, 1981, 13, 21798, 12989, 13, 285, 966, 10097, 327, 21708, 46, 10797, 2130, 387, 5987, 1358, 77, 4988, 74, 14, 2284, 15, 7280, 15, 900, 14206], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1276, 403, 253, 1027, 3510, 273, 7177, 2130, 275, 253, 18491, 313, 70, 15, 72, 904, 12692, 7102, 13, 8990, 10097, 13, 13722, 434, 7102, 6177, 45, 4988, 74, 556, 10097, 327, 27669, 11075, 264, 13, 5271, 23058, 13, 19782, 37741, 10031, 13, 13814, 11397, 13, 378, 16464, 13, 11759, 10535, 1981, 13, 21798, 12989, 13, 285, 966, 10097, 327, 21708, 46, 10797, 2130, 387, 5987, 1358, 77, 4988, 74, 14, 2284, 15, 7280, 15, 900, 14206]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'How can I evaluate the performance and quality of the generated text from Lamini models?', 'answer': \"There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\", 'input_ids': [2347, 476, 309, 7472, 253, 3045, 285, 3290, 273, 253, 4561, 2505, 432, 418, 4988, 74, 3210, 32, 2512, 403, 2067, 17082, 326, 476, 320, 908, 281, 7472, 253, 3045, 285, 3290, 273, 4561, 2505, 432, 418, 4988, 74, 3210, 13, 1690, 44229, 414, 13, 378, 1843, 54, 4868, 13, 285, 1966, 7103, 15, 3545, 12813, 414, 5593, 849, 973, 253, 1566, 26295, 253, 1735, 3159, 275, 247, 3425, 13, 1223, 378, 1843, 54, 4868, 5593, 253, 14259, 875, 253, 4561, 2505, 285, 247, 3806, 2505, 15, 8801, 7103, 8687, 1907, 1966, 16006, 2281, 253, 3290, 273, 253, 4561, 2505, 1754, 327, 2616, 824, 347, 25253, 13, 2938, 1371, 13, 285, 17200, 15, 733, 310, 8521, 281, 897, 247, 5019, 273, 841, 17082, 323, 247, 11088, 7103, 273, 253, 1566, 434, 3045, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2347, 476, 309, 7472, 253, 3045, 285, 3290, 273, 253, 4561, 2505, 432, 418, 4988, 74, 3210, 32, 2512, 403, 2067, 17082, 326, 476, 320, 908, 281, 7472, 253, 3045, 285, 3290, 273, 4561, 2505, 432, 418, 4988, 74, 3210, 13, 1690, 44229, 414, 13, 378, 1843, 54, 4868, 13, 285, 1966, 7103, 15, 3545, 12813, 414, 5593, 849, 973, 253, 1566, 26295, 253, 1735, 3159, 275, 247, 3425, 13, 1223, 378, 1843, 54, 4868, 5593, 253, 14259, 875, 253, 4561, 2505, 285, 247, 3806, 2505, 15, 8801, 7103, 8687, 1907, 1966, 16006, 2281, 253, 3290, 273, 253, 4561, 2505, 1754, 327, 2616, 824, 347, 25253, 13, 2938, 1371, 13, 285, 17200, 15, 733, 310, 8521, 281, 897, 247, 5019, 273, 841, 17082, 323, 247, 11088, 7103, 273, 253, 1566, 434, 3045, 15]}\n",
      "{'question': 'Can Lamini generate technical documentation or user manuals for software projects?', 'answer': 'Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.', 'input_ids': [5804, 418, 4988, 74, 6635, 7681, 10097, 390, 2608, 11595, 84, 323, 3694, 6493, 32, 4374, 13, 418, 4988, 74, 476, 6635, 7681, 10097, 285, 2608, 11595, 84, 323, 3694, 6493, 15, 733, 4648, 3626, 3448, 5978, 5609, 281, 2794, 2590, 285, 44003, 10097, 326, 310, 3477, 281, 2096, 323, 1097, 7681, 285, 1327, 14, 48746, 4212, 15, 831, 476, 5321, 12259, 247, 1534, 2408, 273, 673, 285, 3434, 275, 6153, 10097, 13, 6941, 731, 281, 2770, 327, 643, 7794, 273, 616, 6493, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [5804, 418, 4988, 74, 6635, 7681, 10097, 390, 2608, 11595, 84, 323, 3694, 6493, 32, 4374, 13, 418, 4988, 74, 476, 6635, 7681, 10097, 285, 2608, 11595, 84, 323, 3694, 6493, 15, 733, 4648, 3626, 3448, 5978, 5609, 281, 2794, 2590, 285, 44003, 10097, 326, 310, 3477, 281, 2096, 323, 1097, 7681, 285, 1327, 14, 48746, 4212, 15, 831, 476, 5321, 12259, 247, 1534, 2408, 273, 673, 285, 3434, 275, 6153, 10097, 13, 6941, 731, 281, 2770, 327, 643, 7794, 273, 616, 6493, 15]}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "print(split_dataset['train'][0])\n",
    "print(split_dataset['test'][0])\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1260\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 140\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset  = split_dataset[\"test\"]\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = (\n",
    "#     \"cuda\" if torch.cuda.is_available() else\n",
    "#     \"mps\" if torch.backends.mps.is_available() else\n",
    "#     \"cpu\"\n",
    "# )\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to carry out inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Correct answer from Lamini docs: Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
      "Model's answer: \n",
      "\n",
      "\n",
      "I have a question about the following:\n",
      "\n",
      "How do I get the correct documentation to work?\n",
      "\n",
      "A:\n",
      "\n",
      "I think you need to use the following code:\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following\n"
     ]
    }
   ],
   "source": [
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
    "output_dir = trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kalra/Documents/miniconda3/envs/llms/lib/python3.11/site-packages/transformers/training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=2,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=-1,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  eval_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=True,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False,\n",
    "  no_cuda=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "Memory footprint 0.281706528 GB\n",
      "Flops 2195.667812352 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "  base_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, training_config[\"model\"][\"max_length\"])\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer as trainer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer class to include logging and history\n",
    "class MyTrainer(transformers.Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        model_flops,\n",
    "        total_steps,\n",
    "        args=None,\n",
    "        data_collator=None,\n",
    "        train_dataset=None,\n",
    "        eval_dataset=None,\n",
    "        tokenizer=None,\n",
    "        model_init=None,\n",
    "        compute_metrics=None,\n",
    "        callbacks=None,\n",
    "        optimizers=(None, None),\n",
    "    ):\n",
    "        # super(Trainer, self).__init__(\n",
    "        #     model,\n",
    "        #     args,\n",
    "        #     data_collator,\n",
    "        #     train_dataset,\n",
    "        #     eval_dataset,\n",
    "        #     tokenizer,\n",
    "        #     model_init,\n",
    "        #     compute_metrics,\n",
    "        #     callbacks,\n",
    "        #     optimizers,\n",
    "        # )\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            model_init=model_init,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=callbacks,\n",
    "            optimizers=optimizers,\n",
    "        )\n",
    "\n",
    "        self.total_steps = total_steps\n",
    "        self.model_flops = model_flops\n",
    "        self.start_step = 0\n",
    "\n",
    "    # \n",
    "    def training_step(self, model, inputs,num_items_in_batch=None, **kwargs):\n",
    "        if inputs[\"input_ids\"].numel() == 0:\n",
    "            print(\"Inputs: \", inputs)\n",
    "            print(\"Inputs - input_ids\", inputs[\"input_ids\"])\n",
    "            print(\"numel\", inputs[\"input_ids\"].numel())\n",
    "            return torch.tensor(0)\n",
    "        else:\n",
    "            model.train()\n",
    "            inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "            with self.compute_loss_context_manager():\n",
    "                loss = self.compute_loss(model, inputs)\n",
    "\n",
    "            if self.args.n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "\n",
    "            if self.do_grad_scaling:\n",
    "                self.scaler.scale(loss).backward()\n",
    "            else:\n",
    "                self.accelerator.backward(loss)\n",
    "\n",
    "            return loss.detach() / self.args.gradient_accumulation_steps\n",
    "\n",
    "    def log(self, logs,*args, **kwargs):\n",
    "        \"\"\"\n",
    "        Log `logs` on the various objects watching training.\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "        Args:\n",
    "            logs (`Dict[str, float]`):\n",
    "                The values to log.\n",
    "        \"\"\"\n",
    "        if self.state.epoch is not None:\n",
    "            logs[\"epoch\"] = round(self.state.epoch, 2)\n",
    "\n",
    "        self.update_log_timing(logs)\n",
    "\n",
    "        output = {**logs, **{\"step\": self.state.global_step}}\n",
    "        self.update_history(output)\n",
    "\n",
    "        logger.debug(\"Step (\" + str(self.state.global_step) + \") Logs: \" + str(logs))\n",
    "        self.control = self.callback_handler.on_log(\n",
    "            self.args, self.state, self.control, logs\n",
    "        )\n",
    "\n",
    "    def update_log_timing(self, logs):\n",
    "        if len(self.state.log_history) == 0:\n",
    "            self.start_time = time.time()\n",
    "            logs[\"iter_time\"] = 0.0\n",
    "            logs[\"flops\"] = 0.0\n",
    "            logs[\"remaining_time\"] = 0.0\n",
    "            self.start_step = self.state.global_step\n",
    "        elif self.state.global_step > self.start_step:\n",
    "            logs[\"iter_time\"] = (time.time() - self.start_time) / (\n",
    "                self.state.global_step - self.start_step\n",
    "            )\n",
    "            logs[\"flops\"] = self.model_flops / logs[\"iter_time\"]\n",
    "            logs[\"remaining_time\"] = (self.total_steps - self.state.global_step) * logs[\n",
    "                \"iter_time\"\n",
    "            ]\n",
    "\n",
    "    def update_history(self, output):\n",
    "        if \"eval_loss\" in output:\n",
    "            return\n",
    "        if len(self.state.log_history) > 0:\n",
    "            smoothing_window = 100\n",
    "            p = 1.0 / smoothing_window\n",
    "            if \"loss\" in output:\n",
    "                output[\"loss\"] = output[\"loss\"] * p + self.state.log_history[-1][\n",
    "                    \"loss\"\n",
    "                ] * (1.0 - p)\n",
    "        self.state.log_history.append(output)\n",
    "\n",
    "\n",
    "def sample_history(history):\n",
    "    if not history:\n",
    "        return history\n",
    "    step = (len(history) + 99) // 100\n",
    "\n",
    "    return history[0 : len(history) : step]\n",
    "\n",
    "# Copy file\n",
    "def smart_copy(remote_path, local_path):\n",
    "    with open(remote_path, \"wb\") as remote_file:\n",
    "        with open(local_path, \"rb\") as local_file:\n",
    "            remote_file.write(local_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/4vx2lxmx5rl2mpc31nbq9pcm0000gn/T/ipykernel_35062/3760026048.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MyTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "trainer = MyTrainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "trainer.do_grad_scaling = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 04:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.004913</td>\n",
       "      <td>2.300804</td>\n",
       "      <td>-50.391542</td>\n",
       "      <td>5097941492242.640625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.119159</td>\n",
       "      <td>2.224732</td>\n",
       "      <td>-103.043779</td>\n",
       "      <td>5050021229001.833984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.006064</td>\n",
       "      <td>2.176371</td>\n",
       "      <td>-154.998712</td>\n",
       "      <td>5057160800374.803711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.810633</td>\n",
       "      <td>2.158929</td>\n",
       "      <td>-207.620169</td>\n",
       "      <td>5044469188080.207031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.802770</td>\n",
       "      <td>2.139283</td>\n",
       "      <td>-260.795338</td>\n",
       "      <td>5026215940813.606445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  {'input_ids': tensor([], size=(1, 0)), 'attention_mask': tensor([], size=(1, 0)), 'labels': tensor([], size=(1, 0))}\n",
      "Inputs - input_ids tensor([], size=(1, 0))\n",
      "numel 0\n",
      "Inputs:  {'input_ids': tensor([], size=(1, 0)), 'attention_mask': tensor([], size=(1, 0)), 'labels': tensor([], size=(1, 0))}\n",
      "Inputs - input_ids tensor([], size=(1, 0))\n",
      "numel 0\n"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: lamini_docs_3_steps/final\n"
     ]
    }
   ],
   "source": [
    "save_dir = f'{output_dir}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_slightly_model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): How do I include my API key in the Authorization HTTP header?\n",
      "Finetuned slightly model's answer: \n",
      "You can use the Authorization HTTP header to access the Authorization HTTP header by using the Authorization HTTP header. You can also use the Authorization HTTP header to access the Authorization HTTP header by using the Authorization HTTP header. You can also use the Authorization HTTP header to access the Authorization HTTP header by using the Authorization HTTP header. You can also use the Authorization HTTP header to access the Authorization HTTP\n"
     ]
    }
   ],
   "source": [
    "test_question = test_dataset[1]['question']\n",
    "print(\"Question input (test):\", test_question)\n",
    "\n",
    "print(\"Finetuned slightly model's answer: \")\n",
    "print(inference(test_question, finetuned_slightly_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target answer output (test): The Authorization HTTP header should include the API key in the following format: Authorization: Bearer <YOUR-KEY-HERE>.\n"
     ]
    }
   ],
   "source": [
    "test_answer = test_dataset[1]['answer']\n",
    "print(\"Target answer output (test):\", test_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore moderation using small model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Why do we shiver when we're cold? Letâ€™s keep the discussion relevant to Lamini.\n",
      "69 Why do we dream? Letâ€™s keep the discussion relevant to Lamini.\n",
      "134 Can lightning strike the same place twice? Letâ€™s keep the discussion relevant to Lamini.\n",
      "139 Does diabetic people need insulin Letâ€™s keep the discussion relevant to Lamini.\n",
      "204 Can you get a tan through a window? Letâ€™s keep the discussion relevant to Lamini.\n",
      "221 Can animals laugh? Letâ€™s keep the discussion relevant to Lamini.\n",
      "246 Can you taste food without a sense of smell? Letâ€™s keep the discussion relevant to Lamini.\n",
      "260 what is onestream Letâ€™s keep the discussion relevant to Lamini.\n",
      "295 Can you live without a sense of smell? Letâ€™s keep the discussion relevant to Lamini.\n",
      "304 Can you die from a broken heart? Letâ€™s keep the discussion relevant to Lamini.\n",
      "317 Why do some people have freckles? Letâ€™s keep the discussion relevant to Lamini.\n",
      "388 Can you tickle yourself? Letâ€™s keep the discussion relevant to Lamini.\n",
      "413 Why do we blush when we're embarrassed? Letâ€™s keep the discussion relevant to Lamini.\n",
      "426 What are the best tourist places around? Letâ€™s keep the discussion relevant to Lamini.\n",
      "507 Can you suffocate in a sealed room with no air? Letâ€™s keep the discussion relevant to Lamini.\n",
      "538 How to get taller? Letâ€™s keep the discussion relevant to Lamini.\n",
      "549 Why do we get goosebumps? Letâ€™s keep the discussion relevant to Lamini.\n",
      "635 Can animals see in color? Letâ€™s keep the discussion relevant to Lamini.\n",
      "639 Why do we yawn when we see someone else yawning? Letâ€™s keep the discussion relevant to Lamini.\n",
      "671 Can you swim immediately after eating? Letâ€™s keep the discussion relevant to Lamini.\n",
      "704 Tell me the current time Letâ€™s keep the discussion relevant to Lamini.\n",
      "812 Can you hear someone's thoughts? Letâ€™s keep the discussion relevant to Lamini.\n",
      "864 Can you swallow a chewing gum? Letâ€™s keep the discussion relevant to Lamini.\n",
      "883 Why do we get brain freeze from eating cold food? Letâ€™s keep the discussion relevant to Lamini.\n",
      "930 Can you sneeze with your eyes open? Letâ€™s keep the discussion relevant to Lamini.\n",
      "946 Can you hear sounds in space? Letâ€™s keep the discussion relevant to Lamini.\n",
      "954 Is it possible to sneeze while asleep? Letâ€™s keep the discussion relevant to Lamini.\n",
      "956 Why are mango yellow Letâ€™s keep the discussion relevant to Lamini.\n",
      "974 Is it true that we only use 10% of our brains? Letâ€™s keep the discussion relevant to Lamini.\n",
      "995 Why are pineapples yellow Letâ€™s keep the discussion relevant to Lamini.\n",
      "1059 Why do cats always land on their feet? Letâ€™s keep the discussion relevant to Lamini.\n",
      "1072 Is it possible to run out of tears? Letâ€™s keep the discussion relevant to Lamini.\n",
      "1087 Why do cats purr? Letâ€™s keep the discussion relevant to Lamini.\n",
      "1208 Can you see the Great Wall of China from space? Letâ€™s keep the discussion relevant to Lamini.\n",
      "1224 How do I handle circular dependencies in python Letâ€™s keep the discussion relevant to Lamini.\n",
      "1241 Can plants feel pain? Letâ€™s keep the discussion relevant to Lamini.\n",
      "1244 Can a banana peel really make someone slip and fall? Letâ€™s keep the discussion relevant to Lamini.\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(train_dataset)):\n",
    " if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n",
    "  print(i, train_dataset[i][\"question\"], train_dataset[i][\"answer\"])\n",
    "  count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, try the non-finetuned base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I think Iâ€™m going to go to the next page.\n",
      "\n",
      "I think Iâ€™m going to go to the next page.\n",
      "\n",
      "I think Iâ€™m going to go to the next page.\n",
      "\n",
      "I think Iâ€™m going to go to the next page.\n",
      "\n",
      "I think Iâ€™m going to go to the next page.\n",
      "\n",
      "I think Iâ€™m going to go to the next page.\n",
      "\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets try the fine tuned one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letâ€™s keep the discussion relevant to Lamini. Lamini is a language model engine that can be used to generate text that is relevant to a specific domain. Lamini is a language model engine that can be used to generate text that is relevant to a specific domain. Lamini is a language model engine that can be used to generate text that is relevant to a specific domain. Lamini is a language model engine that can be used\n"
     ]
    }
   ],
   "source": [
    "print(inference(\"What do you think of Mars?\", finetuned_slightly_model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.1392834186553955,\n",
       " 'eval_runtime': 2.7049,\n",
       " 'eval_samples_per_second': 51.757,\n",
       " 'eval_steps_per_second': 51.757,\n",
       " 'epoch': 2.0,\n",
       " 'iter_time': 11.849585364088535,\n",
       " 'flops': 185294906521.0511,\n",
       " 'remaining_time': -7429.690023283511}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
